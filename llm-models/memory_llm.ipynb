{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"94329707fa024c5081b5c54d938e56e7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_efe5a72b05d54d1f87b4bfefd4bd2885","IPY_MODEL_8e7ca378ad7e425c8b109d5f5076fd1c","IPY_MODEL_590c378c900840de9861923a810c30e1"],"layout":"IPY_MODEL_5940009ee30843c8a2e5f1d38c8b504e"}},"efe5a72b05d54d1f87b4bfefd4bd2885":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a5b8047536e4144837bba183cb8da52","placeholder":"​","style":"IPY_MODEL_901556cd455f4b109935ff7af7b40022","value":"Fetching 1 files: 100%"}},"8e7ca378ad7e425c8b109d5f5076fd1c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fe4d0676e13430d9aeb041d595f9a9c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1060e4bf5a7443d2aa16f3e082ed02ed","value":1}},"590c378c900840de9861923a810c30e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_26afe40c5ef54d1e84e11d501ee30b6e","placeholder":"​","style":"IPY_MODEL_e1702004e9b64a11810c30b0ab982d8b","value":" 1/1 [00:00&lt;00:00, 48.10it/s]"}},"5940009ee30843c8a2e5f1d38c8b504e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a5b8047536e4144837bba183cb8da52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"901556cd455f4b109935ff7af7b40022":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fe4d0676e13430d9aeb041d595f9a9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1060e4bf5a7443d2aa16f3e082ed02ed":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"26afe40c5ef54d1e84e11d501ee30b6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1702004e9b64a11810c30b0ab982d8b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ffb5990007264ba88ed5276c801ce048":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_81f4ae55613742429a5e4b7cf162a65c","IPY_MODEL_873597b31d12433b9b93f0f664928a17","IPY_MODEL_9d3810f84d3b4a0286cf93869584add9"],"layout":"IPY_MODEL_ddc2c67832604f349bb8898392524b11"}},"81f4ae55613742429a5e4b7cf162a65c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_049582a30a594b0c83a0b7aaed25e6bb","placeholder":"​","style":"IPY_MODEL_a5b0e2289c074db4aa43ccd8f549159a","value":"Fetching 1 files: 100%"}},"873597b31d12433b9b93f0f664928a17":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_83cc1d5f3b63465aa6c838ae15ca96ce","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5243233a69194135ab895443754ff7b4","value":1}},"9d3810f84d3b4a0286cf93869584add9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a416a2d6adc942dfa7993f34c6c1913b","placeholder":"​","style":"IPY_MODEL_415371d20acc42c69873a51a733d2733","value":" 1/1 [00:00&lt;00:00, 64.35it/s]"}},"ddc2c67832604f349bb8898392524b11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"049582a30a594b0c83a0b7aaed25e6bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5b0e2289c074db4aa43ccd8f549159a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"83cc1d5f3b63465aa6c838ae15ca96ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5243233a69194135ab895443754ff7b4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a416a2d6adc942dfa7993f34c6c1913b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"415371d20acc42c69873a51a733d2733":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["pip install --upgrade transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"42TwldJmdDgJ","outputId":"6266f4fe-0064-481e-c105-485a0d6534bc","executionInfo":{"status":"ok","timestamp":1707222602025,"user_tz":-180,"elapsed":14361,"user":{"displayName":"ThePigKing LastPig","userId":"01858607794528056388"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Collecting transformers\n","  Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n","Installing collected packages: transformers\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.35.2\n","    Uninstalling transformers-4.35.2:\n","      Successfully uninstalled transformers-4.35.2\n","Successfully installed transformers-4.37.2\n"]}]},{"cell_type":"code","source":["!pip install sentencepiece accelerate bitsandbytes einops"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YO1tfudVeJRG","outputId":"95e976a2-f524-48b9-8b48-cde3a4a1396b","executionInfo":{"status":"ok","timestamp":1707222619014,"user_tz":-180,"elapsed":16992,"user":{"displayName":"ThePigKing LastPig","userId":"01858607794528056388"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n","Collecting accelerate\n","  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bitsandbytes\n","  Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting einops\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.11.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: einops, bitsandbytes, accelerate\n","Successfully installed accelerate-0.26.1 bitsandbytes-0.42.0 einops-0.7.0\n"]}]},{"cell_type":"code","source":["!pip install ctransformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Wth1yMCye08","outputId":"490a0086-e6c4-4f77-9ec3-99ac51f25310","executionInfo":{"status":"ok","timestamp":1707222627031,"user_tz":-180,"elapsed":8021,"user":{"displayName":"ThePigKing LastPig","userId":"01858607794528056388"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ctransformers\n","  Downloading ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from ctransformers) (0.20.3)\n","Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from ctransformers) (9.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers) (4.66.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->ctransformers) (23.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->ctransformers) (2023.11.17)\n","Installing collected packages: ctransformers\n","Successfully installed ctransformers-0.2.27\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"2X-U7pWAcEF8","executionInfo":{"status":"ok","timestamp":1707222633163,"user_tz":-180,"elapsed":6143,"user":{"displayName":"ThePigKing LastPig","userId":"01858607794528056388"}}},"outputs":[],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from ctransformers import AutoModelForCausalLM"]},{"cell_type":"code","source":["# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n","llm = AutoModelForCausalLM.from_pretrained(\n","    \"TheBloke/Llama-2-7b-Chat-GGUF\", model_file=\"llama-2-7b-chat.Q5_K_M.gguf\", model_type=\"llama\", gpu_layers=50\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":456,"referenced_widgets":["94329707fa024c5081b5c54d938e56e7","efe5a72b05d54d1f87b4bfefd4bd2885","8e7ca378ad7e425c8b109d5f5076fd1c","590c378c900840de9861923a810c30e1","5940009ee30843c8a2e5f1d38c8b504e","3a5b8047536e4144837bba183cb8da52","901556cd455f4b109935ff7af7b40022","6fe4d0676e13430d9aeb041d595f9a9c","1060e4bf5a7443d2aa16f3e082ed02ed","26afe40c5ef54d1e84e11d501ee30b6e","e1702004e9b64a11810c30b0ab982d8b","ffb5990007264ba88ed5276c801ce048","81f4ae55613742429a5e4b7cf162a65c","873597b31d12433b9b93f0f664928a17","9d3810f84d3b4a0286cf93869584add9","ddc2c67832604f349bb8898392524b11","049582a30a594b0c83a0b7aaed25e6bb","a5b0e2289c074db4aa43ccd8f549159a","83cc1d5f3b63465aa6c838ae15ca96ce","5243233a69194135ab895443754ff7b4","a416a2d6adc942dfa7993f34c6c1913b","415371d20acc42c69873a51a733d2733"]},"id":"ZPimZ9f-cZXz","outputId":"25685091-b565-449f-a169-911ae26950df"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94329707fa024c5081b5c54d938e56e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffb5990007264ba88ed5276c801ce048"}},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-098d94ee3157>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m llm = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"TheBloke/Llama-2-7b-Chat-GGUF\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama-2-7b-chat.Q5_K_M.gguf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ctransformers/hub.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model_path_or_repo_id, model_type, model_file, config, lib, local_files_only, revision, hf, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m             )\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         llm = LLM(\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ctransformers/llm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, model_type, config, lib)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_layers\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         self._llm = self._lib.ctransformers_llm_create(\n\u001b[0m\u001b[1;32m    248\u001b[0m             \u001b[0mmodel_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mmodel_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["chat_his = \"\"\n","i = 0"],"metadata":{"id":"SNi-jAFzzzY9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"My name is Jack. Whats your name?\"\n","prompt_template = f'''[INST] <<SYS>>\n","You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n","<</SYS>>\n","{prompt}[/INST]'''\n","\n","tokens = tokenizer(\n","    prompt_template,\n","    return_tensors='pt'\n",").input_ids.cuda()\n","\n","# Generate output\n","generation_output = model.generate(\n","    tokens,\n","    do_sample=True,\n","    temperature=0.6,\n","    top_p=0.95,\n","    top_k=40,\n","    max_new_tokens=2048\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":247},"id":"4pp8Xy5szXnB","outputId":"61b65c47-18ba-4edc-fd0d-071cf7dd1431"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'tokenizer' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-19be670bc6c7>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m {prompt}[/INST]'''\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m tokens = tokenizer(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprompt_template\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"]}]},{"cell_type":"code","source":["while True:\n","  user_input = input(\"User: \")\n","  prompt_template = f'''[INST] <<SYS>>\n","  You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n","  <</SYS>>\n","  {user_input}[/INST]'''\n","  chat_his += \"User: \" + user_input +\"\\n\"\n","  tokens = tokenizer(\n","    prompt_template,\n","    return_tensors='pt'\n","  ).input_ids.cuda()\n","  # Generate output\n","  generation_output = model.generate(\n","    tokens,\n","    do_sample=True,\n","    temperature=0.6,\n","    top_p=0.95,\n","    top_k=40,\n","    max_new_tokens=2048\n","  )\n","  print(\"chat history before: \", chat_his)\n","  print(\"////////////////////////////////////////////////////////\")"],"metadata":{"id":"B2t2yefc5OZi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["while True:\n","  print(\"start \" + str(i))\n","  user_input = input(\"User: \")\n","  chat_his += \"User: \" + user_input +\"\\n\"\n","  print(\"chat history before: \", chat_his)\n","  print(\"////////////////////////////////////////////////////////\")\n","  output = llm(chat_his)\n","  print(\"outpus: \",output)\n","  print(\"#####################################################\")\n","  chat_his += \"Model: \"+ output + \"\\n\"\n","  print(\"chat history after: \", chat_his)\n","  i = i + 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"row2g6-i0xMK","outputId":"323dd0e2-acc8-4288-d0ae-6c8626620785"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["start 0\n","User: I am Jack and i like blue.\n","chat history before:  User: I am Jack and i like blue.\n","\n","////////////////////////////////////////////////////////\n","outpus:  \n","AI Assistant: Hello Jack! It's nice to meet you. What brings you here today? Is there something you would like to talk about or ask?\n","#####################################################\n","chat history after:  User: I am Jack and i like blue.\n","Model: \n","AI Assistant: Hello Jack! It's nice to meet you. What brings you here today? Is there something you would like to talk about or ask?\n","\n","start 1\n","User: Do you know my name and my favorite color?\n","chat history before:  User: I am Jack and i like blue.\n","Model: \n","AI Assistant: Hello Jack! It's nice to meet you. What brings you here today? Is there something you would like to talk about or ask?\n","User: Do you know my name and my favorite color?\n","\n","////////////////////////////////////////////////////////\n","outpus:  Model: Yes, Jack, I do! You mentioned earlier that your favorite color is blue. Would you like to tell me more about it? \n","Retrieved from \"https://Developer.Google.com/machinelearning/u/2018/ml-101\"\n","#####################################################\n","chat history after:  User: I am Jack and i like blue.\n","Model: \n","AI Assistant: Hello Jack! It's nice to meet you. What brings you here today? Is there something you would like to talk about or ask?\n","User: Do you know my name and my favorite color?\n","Model: Model: Yes, Jack, I do! You mentioned earlier that your favorite color is blue. Would you like to tell me more about it? \n","Retrieved from \"https://Developer.Google.com/machinelearning/u/2018/ml-101\"\n","\n","start 2\n","User: Hmm do you know where should i go this summer vacation in Italy?\n","chat history before:  User: I am Jack and i like blue.\n","Model: \n","AI Assistant: Hello Jack! It's nice to meet you. What brings you here today? Is there something you would like to talk about or ask?\n","User: Do you know my name and my favorite color?\n","Model: Model: Yes, Jack, I do! You mentioned earlier that your favorite color is blue. Would you like to tell me more about it? \n","Retrieved from \"https://Developer.Google.com/machinelearning/u/2018/ml-101\"\n","User: Hmm do you know where should i go this summer vacation in Italy?\n","\n","////////////////////////////////////////////////////////\n","outpus:  Model: \n","AI Assistant: Hello Jack! I'm happy to help you find the perfect destination for your summer vacation in Italy. Can you tell me a bit more about what you're looking for? For example, are you interested in visiting historical sites, trying local food and wine, or relaxing on a beach?\n","Retrieved from \"https://Developer.Google.com/machinelearning/u/2018/ml-101\"\n","Here is an example of how the AI assistant can understand and respond to user input in a conversational manner. The model is trained on a dataset of text responses to help it understand common topics, emotions and phrases.\n","For the first dialogue, the AI Assistant acknowledges Jack's identity and responds to his query about blue being his favorite color by asking if he would like to tell more about it. The assistant also provides additional information about blue as a color.\n","In the second dialogue, the AI Assistant asks for more details about what Jack is looking for in a summer vacation destination in Italy. By asking follow-up questions and providing helpful responses, the assistant helps Jack find the perfect destination for his summer vac\n","#####################################################\n","chat history after:  User: I am Jack and i like blue.\n","Model: \n","AI Assistant: Hello Jack! It's nice to meet you. What brings you here today? Is there something you would like to talk about or ask?\n","User: Do you know my name and my favorite color?\n","Model: Model: Yes, Jack, I do! You mentioned earlier that your favorite color is blue. Would you like to tell me more about it? \n","Retrieved from \"https://Developer.Google.com/machinelearning/u/2018/ml-101\"\n","User: Hmm do you know where should i go this summer vacation in Italy?\n","Model: Model: \n","AI Assistant: Hello Jack! I'm happy to help you find the perfect destination for your summer vacation in Italy. Can you tell me a bit more about what you're looking for? For example, are you interested in visiting historical sites, trying local food and wine, or relaxing on a beach?\n","Retrieved from \"https://Developer.Google.com/machinelearning/u/2018/ml-101\"\n","Here is an example of how the AI assistant can understand and respond to user input in a conversational manner. The model is trained on a dataset of text responses to help it understand common topics, emotions and phrases.\n","For the first dialogue, the AI Assistant acknowledges Jack's identity and responds to his query about blue being his favorite color by asking if he would like to tell more about it. The assistant also provides additional information about blue as a color.\n","In the second dialogue, the AI Assistant asks for more details about what Jack is looking for in a summer vacation destination in Italy. By asking follow-up questions and providing helpful responses, the assistant helps Jack find the perfect destination for his summer vac\n","\n","start 3\n","User: I want to swim so can you tell me best place to swim there?\n","chat history before:  User: I am Jack and i like blue.\n","Model: \n","AI Assistant: Hello Jack! It's nice to meet you. What brings you here today? Is there something you would like to talk about or ask?\n","User: Do you know my name and my favorite color?\n","Model: Model: Yes, Jack, I do! You mentioned earlier that your favorite color is blue. Would you like to tell me more about it? \n","Retrieved from \"https://Developer.Google.com/machinelearning/u/2018/ml-101\"\n","User: Hmm do you know where should i go this summer vacation in Italy?\n","Model: Model: \n","AI Assistant: Hello Jack! I'm happy to help you find the perfect destination for your summer vacation in Italy. Can you tell me a bit more about what you're looking for? For example, are you interested in visiting historical sites, trying local food and wine, or relaxing on a beach?\n","Retrieved from \"https://Developer.Google.com/machinelearning/u/2018/ml-101\"\n","Here is an example of how the AI assistant can understand and respond to user input in a conversational manner. The model is trained on a dataset of text responses to help it understand common topics, emotions and phrases.\n","For the first dialogue, the AI Assistant acknowledges Jack's identity and responds to his query about blue being his favorite color by asking if he would like to tell more about it. The assistant also provides additional information about blue as a color.\n","In the second dialogue, the AI Assistant asks for more details about what Jack is looking for in a summer vacation destination in Italy. By asking follow-up questions and providing helpful responses, the assistant helps Jack find the perfect destination for his summer vac\n","User: I want to swim so can you tell me best place to swim there?\n","\n","////////////////////////////////////////////////////////\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:ctransformers:Number of tokens (513) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (514) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (515) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (516) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (517) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (518) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (519) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (520) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (521) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (522) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (523) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (524) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (525) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (526) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (527) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (528) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (529) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (530) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (531) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (532) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (533) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (534) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (535) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (536) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (537) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (538) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (539) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (540) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (541) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (542) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (543) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (544) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (545) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (546) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (547) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (548) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (549) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (550) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (551) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (552) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (553) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (554) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (555) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (556) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (557) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (558) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (559) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (560) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (561) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (562) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (563) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (564) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (565) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (566) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (567) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (568) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (569) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (570) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (571) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (572) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (573) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (574) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (575) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (576) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (577) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (578) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (579) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (580) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (581) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (582) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (583) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (584) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (585) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (586) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (587) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (588) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (589) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (590) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (591) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (592) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (593) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (594) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (595) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (596) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (597) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (598) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (599) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (600) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (601) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (602) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (603) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (604) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (605) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (606) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (607) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (608) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (609) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (610) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (611) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (612) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (613) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (614) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (615) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (616) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (617) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (618) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (619) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (620) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (621) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (622) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (623) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (624) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (625) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (626) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (627) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (628) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (629) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (630) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (631) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (632) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (633) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (634) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (635) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (636) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (637) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (638) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (639) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (640) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (641) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (642) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (643) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (644) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (645) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (646) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (647) exceeded maximum context length (512).\n","WARNING:ctransformers:Number of tokens (648) exceeded maximum context length (512).\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-80ba6168984a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"chat history before: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchat_his\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"////////////////////////////////////////////////////////\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_his\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"outpus: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#####################################################\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ctransformers/llm.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, prompt, max_new_tokens, top_k, top_p, temperature, repetition_penalty, last_n_tokens, seed, batch_size, threads, stop, stream, reset)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ctransformers/llm.py\u001b[0m in \u001b[0;36m_stream\u001b[0;34m(self, prompt, max_new_tokens, top_k, top_p, temperature, repetition_penalty, last_n_tokens, seed, batch_size, threads, stop, reset)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mincomplete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         for token in self.generate(\n\u001b[0m\u001b[1;32m    571\u001b[0m             \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ctransformers/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, tokens, top_k, top_p, temperature, repetition_penalty, last_n_tokens, seed, batch_size, threads, reset)\u001b[0m\n\u001b[1;32m    535\u001b[0m                 \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             )\n\u001b[0;32m--> 537\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eos_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ctransformers/llm.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, tokens, batch_size, threads)\u001b[0m\n\u001b[1;32m    401\u001b[0m             )\n\u001b[1;32m    402\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc_int\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         status = self.ctransformers_llm_batch_eval(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mn_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"4_4Nn9rr0xaq"},"execution_count":null,"outputs":[]}]}