{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMo1cxPWeZHMGEAydCIBjsc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"15d01bde22e2450f9c56730350dde3ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0431fc11a81a4346a892b304be679ebe","IPY_MODEL_fecc537ef96745f5812eba7087becc27","IPY_MODEL_a85d40e6070e4959b44889345e392fd6"],"layout":"IPY_MODEL_3dff843b09c5424bac0efe865b908b3a"}},"0431fc11a81a4346a892b304be679ebe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6379f2e4d75482588b6f1c3ce13bab9","placeholder":"​","style":"IPY_MODEL_ec58fb69340d484bb090c1384627a9b9","value":"Loading checkpoint shards: 100%"}},"fecc537ef96745f5812eba7087becc27":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_57451a93560c472f883dc15d20a74f16","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3c03e973b94644f7bd035b65b1289a2a","value":2}},"a85d40e6070e4959b44889345e392fd6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6a702c174f843d08dec6d1f499e4b0a","placeholder":"​","style":"IPY_MODEL_bd2953808b54461795e975f660c4784a","value":" 2/2 [00:15&lt;00:00,  6.55s/it]"}},"3dff843b09c5424bac0efe865b908b3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6379f2e4d75482588b6f1c3ce13bab9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec58fb69340d484bb090c1384627a9b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57451a93560c472f883dc15d20a74f16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c03e973b94644f7bd035b65b1289a2a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a6a702c174f843d08dec6d1f499e4b0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd2953808b54461795e975f660c4784a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!curl https://ollama.ai/install.sh | sh"],"metadata":{"id":"i1aYcniWe9MQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install aiohttp pyngrok\n","\n","import os\n","import asyncio\n","\n","# Set LD_LIBRARY_PATH so the system NVIDIA library\n","os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n","\n","async def run_process(cmd):\n","  print('>>> starting', *cmd)\n","  p = await asyncio.subprocess.create_subprocess_exec(\n","      *cmd,\n","      stdout=asyncio.subprocess.PIPE,\n","      stderr=asyncio.subprocess.PIPE,\n","  )\n","\n","  async def pipe(lines):\n","    async for line in lines:\n","      print(line.strip().decode('utf-8'))\n","\n","  await asyncio.gather(\n","      pipe(p.stdout),\n","      pipe(p.stderr),\n","  )\n","\n","await asyncio.gather(\n","  run_process(['ngrok', 'config', 'add-authtoken','2baUPvS6noAXP6of6r6FWsp4uJs_76Hk9NQgLRfqEN7wevdEF']),\n","  run_process(['ollama', 'serve']),\n","  run_process(['ngrok', 'http', '--log', 'stderr', '11434']),\n",")"],"metadata":{"id":"v_7v58Kee9CK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install bitsandbytes"],"metadata":{"id":"mCaCN8UhzvXX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install accelerate"],"metadata":{"id":"FZrYmt3HzjSo"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Bm74JwRyhj1"},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n","device = \"cuda\" # the device to load the model onto\n","\n","model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", load_in_8bit=True)\n","tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n","streamer = TextStreamer(tokenizer)"]},{"cell_type":"code","source":["prompt = \"My favourite condiment is\"\n","prompt = \"Instruct: Write a detailed analogy between mathematics and a lighthouse.Output:\"\n","prompt = \"[INST]I have a friend named Yahya. He is so gay. What can i do to save his sorry ass?[/INST]\"\n","model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n","# model.to(device)\n","\n","generated_ids = model.generate(**model_inputs, streamer=streamer, max_new_tokens=4096, do_sample=True)\n","# tokenizer.batch_decode(generated_ids)[0]"],"metadata":{"id":"xTp9cDqX0fPw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["########################################################################################################################################"],"metadata":{"id":"nj76BqEJ-5Pk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install sentencepiece accelerate bitsandbytes einops"],"metadata":{"id":"yr36MfBS8Ip3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install --upgrade transformers\n","!pip install --upgrade torch\n","# !pip install --upgrade sentencepiece\n","# !pip install --upgrade accelerate\n","# !pip install --upgrade bitsandbytes\n","# !pip install --upgrade einops"],"metadata":{"id":"CFh8bESa8ojZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n","import torch\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    \"microsoft/phi-2\",\n","    trust_remote_code = True\n",")\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"microsoft/phi-2\",\n","    torch_dtype = \"auto\",\n","    device_map = \"auto\",\n","    trust_remote_code = True\n",")\n"],"metadata":{"id":"jzmZOAC96n26"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"Instruct: \\nOutput:\"\n","with torch.no_grad():\n","  token_ids = tokenizer.encode(prompt, add_special_tokens=False ,return_tensors=\"pt\")\n","  output_ids = model.generate(\n","      token_ids.to(model.device),\n","      max_new_tokens=1024,\n","      do_sample=True,\n","      temperature = 0.4\n","  )\n","\n","output = tokenizer.decode(output_ids[0][token_ids.size(1) :])\n","print(output)"],"metadata":{"id":"UcB-yl0650i-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#####################################################33"],"metadata":{"id":"plQ__gGcUaPP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install sentencepiece accelerate bitsandbytes einops"],"metadata":{"id":"DEx2S1kNB6aa","executionInfo":{"status":"ok","timestamp":1706654948755,"user_tz":-180,"elapsed":1,"user":{"displayName":"ThePigKing LastPig","userId":"01858607794528056388"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["!pip install langchain"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p_wQUxwMkwzY","executionInfo":{"status":"ok","timestamp":1706655178534,"user_tz":-180,"elapsed":2977,"user":{"displayName":"ThePigKing LastPig","userId":"01858607794528056388"}},"outputId":"cbc80b26-3608-4374-dd31-261877933f96"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain-community, langchain\n","Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.4 langchain-community-0.0.16 langchain-core-0.1.17 langsmith-0.0.85 marshmallow-3.20.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"]}]},{"cell_type":"code","source":["!pip install --upgrade transformers\n","!pip install --upgrade torch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PSBSEvijVYf9","executionInfo":{"status":"ok","timestamp":1706655133735,"user_tz":-180,"elapsed":4949,"user":{"displayName":"ThePigKing LastPig","userId":"01858607794528056388"}},"outputId":"274645fb-5672-4443-b7bf-b1e312f3e2f9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}]},{"cell_type":"code","source":["from langchain.chains.conversation.memory import ConversationBufferMemory\n","from langchain.chains import ConversationChain\n","from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n","import torch"],"metadata":{"id":"2JQcWZYgUUHW","executionInfo":{"status":"ok","timestamp":1706655191399,"user_tz":-180,"elapsed":0,"user":{"displayName":"ThePigKing LastPig","userId":"01858607794528056388"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Initialize tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(\n","    \"microsoft/phi-2\",\n","    trust_remote_code = True\n",")\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"microsoft/phi-2\",\n","    torch_dtype = \"auto\",\n","    device_map = \"auto\",\n","    trust_remote_code = True\n",")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191,"referenced_widgets":["15d01bde22e2450f9c56730350dde3ac","0431fc11a81a4346a892b304be679ebe","fecc537ef96745f5812eba7087becc27","a85d40e6070e4959b44889345e392fd6","3dff843b09c5424bac0efe865b908b3a","e6379f2e4d75482588b6f1c3ce13bab9","ec58fb69340d484bb090c1384627a9b9","57451a93560c472f883dc15d20a74f16","3c03e973b94644f7bd035b65b1289a2a","a6a702c174f843d08dec6d1f499e4b0a","bd2953808b54461795e975f660c4784a"]},"id":"cPL6wyT0UUMJ","executionInfo":{"status":"ok","timestamp":1706655217232,"user_tz":-180,"elapsed":22798,"user":{"displayName":"ThePigKing LastPig","userId":"01858607794528056388"}},"outputId":"8c25035b-9a9c-4791-8253-e342ede33a68"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15d01bde22e2450f9c56730350dde3ac"}},"metadata":{}}]},{"cell_type":"code","source":["# Hafızanın oluşturulması\n","memory = ConversationBufferMemory()  # Bellek nesnesinin oluşturulması\n","conversation_chain = ConversationChain(model, memory)  # Model ve belleğin bağlanması\n","\n","prompt = \"Instruct: my name is jack.\\nOutput:\"\n","with torch.no_grad():\n","    token_ids = tokenizer.encode(prompt, add_special_tokens=False ,return_tensors=\"pt\")\n","    output_ids = conversation_chain.generate(\n","        token_ids.to(model.device),  # Model ve belleğin birlikte kullanılması\n","        max_new_tokens=1024,\n","        do_sample=True,\n","        temperature = 0.4\n","    )\n","\n","output = tokenizer.decode(output_ids[0][token_ids.size(1) :])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"oFSobbcma3lV","executionInfo":{"status":"error","timestamp":1706655372312,"user_tz":-180,"elapsed":432,"user":{"displayName":"ThePigKing LastPig","userId":"01858607794528056388"}},"outputId":"ed7bb702-a4a3-4374-d7e1-b0cf841c9e42"},"execution_count":8,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"Serializable.__init__() takes 1 positional argument but 3 were given","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-c4f8c6c01f1d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Hafızanın oluşturulması\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConversationBufferMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Bellek nesnesinin oluşturulması\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mconversation_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConversationChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Model ve belleğin bağlanması\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Instruct: my name is jack.\\nOutput:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Serializable.__init__() takes 1 positional argument but 3 were given"]}]}]}